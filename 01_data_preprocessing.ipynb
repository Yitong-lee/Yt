import pandas as pd
import numpy as np
from sklearn.utils import shuffle
df = pd.read_csv('train_reviews.csv',encoding='ISO-8859-1')
df.shape # observate the shape of the trainset
df=df.iloc[:,0:11] # keep valid column
df.info() # Look at the data type

# Check rows whether misaligned data or outliers exist
print(df['review_rating'].value_counts())
print(df['number_of_photos'].value_counts())
print(df['fake_review'].value_counts())

df = df.astype(str)

# delete rows where review rating is not between 0 and 5.
df['length'] = df.review_rating.str.len()
df = df[df.length ==1]

# delete rows where review body and review date are stuck together.
df=df[df["fake_review"].isin(['0','1'])]

# check again whether there are rows with misaligned data. we found that each numeric variables has reasonable rariance. 
print(df['review_rating'].value_counts())
print(df['number_of_photos'].value_counts())
print(df['helpful_vote'].value_counts())
print(df['fake_asin'].value_counts())
print(df['fake_review'].value_counts())
#delete "length" column since it is unncessary.
df=df.drop('length',axis=1)

# format the data into appropriate types
df['review_rating']=df['review_rating'].astype('int')
df['number_of_photos']=df['number_of_photos'].astype('int')
df['helpful_vote']=df['helpful_vote'].astype('int')
df['fake_review']=df['fake_review'].astype('int')
df['fake_asin']=df['fake_asin'].astype('int')
df['product_ID']=df['product_ID'].astype('float').astype('int')
df['review_ID']=df['review_ID'].astype('float').astype('int')

#combine review title and review body
df['review'] = df['review_title'] + ' '+df['review_body']

# replace review columns null with NA
df['review'] = df['review'].fillna('NA')

# convert 'date' column to datetime object
df['review_date'] = pd.to_datetime(df['review_date'])

# create new 'year' column extracted from 'date' column
df['year'] = df['review_date'].dt.year
df['month'] = df['review_date'].dt.month
df['day'] = df['review_date'].dt.day

# create "is_weekday" to represent whether reviews are on weekends.
df['is_weekday'] = pd.to_datetime(df['review_date']).dt.dayofweek.apply(lambda x: 1 if x < 5 else 0)

# drop the column: review_title, review_body, review_date
df = df.drop(["review_title", "review_body"], axis = 1)

import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
import warnings
warnings.filterwarnings('ignore')
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
import string, nltk
from nltk import word_tokenize
from nltk.stem.snowball import SnowballStemmer
nltk.download('wordnet')
nltk.download('omw-1.4')

import nltk
nltk.download('stopwords')
import nltk
nltk.download('punkt')

df.isnull().sum() # there is no Null value in df

# delete all gibberish in the review column
import re
df['review'] = df['review'].replace({r'[^a-zA-Z0-9\s\']':''}, regex=True)
#delete punctuation in review column
df['review'] = df['review'].apply(lambda x: re.sub(r'[^\w\s\']', '', x) if isinstance(x, str) else '')

# format text data into lower structure.
df['review'] = df['review'].str.lower()

# Define a function to remove stopwords
def remove_stopwords(text):
    stop_words = set(stopwords.words('english'))
    words = nltk.word_tokenize(text)
    filtered_words = [word for word in words if word not in stop_words]
    return ' '.join(filtered_words)

# Apply the function to the text column of the dataframe
df['review'] = df['review'].apply(remove_stopwords)

stemmer = SnowballStemmer(language='english')

# apply snowball stemming to eliminate affixes
def stem_words(text):
    words = nltk.word_tokenize(text)
    stemmed_words = [stemmer.stem(word) for word in words]
    return ' '.join(stemmed_words)
# Apply the function to the text column of the dataframe
df['review'] = df['review'].apply(stem_words)

from textblob import TextBlob
# create a function to compute the sentiment score of a text input
def get_sentiment(text):
    blob = TextBlob(text)
    return blob.sentiment.polarity, blob.sentiment.subjectivity

df[['polarity', 'subjectivity']] = df['review'].apply(get_sentiment).apply(pd.Series)

# create a new variable "labeled_product_id" to label the product with fake review assumption. 
df['labeled_product_id'] = df['fake_asin'] * df ['product_ID']

df.to_csv('train_cleaned.csv', index=False)

test = pd.read_csv('test_reviews.csv',encoding='ISO-8859-1')
test['review'] = test['review_title'] + ' '+test['review_body']
test['review'] = test['review'].fillna('NA')
# convert 'date' column to datetime object
test['review_date'] = pd.to_datetime(test['review_date'])

# create new 'year' column extracted from 'date' column
test['year'] = test['review_date'].dt.year
test['month'] = test['review_date'].dt.month
test['day'] = test['review_date'].dt.day

test['is_weekday'] = pd.to_datetime(test['review_date']).dt.dayofweek.apply(lambda x: 1 if x < 5 else 0)
test = test.drop(["review_title", "review_body","review_date"], axis = 1)
# delete all gibberish in the review column
test['review'] = test['review'].replace({r'[^a-zA-Z0-9\s\']':''}, regex=True)
#delete punctuation in review column
test['review'] = test['review'].apply(lambda x: re.sub(r'[^\w\s\']', '', x) if isinstance(x, str) else '')
test['review'] = test['review'].str.lower()
# Define a function to remove stopwords
def remove_stopwords(text):
    stop_words = set(stopwords.words('english'))
    words = nltk.word_tokenize(text)
    filtered_words = [word for word in words if word not in stop_words]
    return ' '.join(filtered_words)

# Apply the function to the text column of the dataframe
test['review'] = test['review'].apply(remove_stopwords)

# apply snowball stemming to eliminate affixes
def stem_words(text):
    words = nltk.word_tokenize(text)
    stemmed_words = [stemmer.stem(word) for word in words]
    return ' '.join(stemmed_words)

# Apply the function to the text column of the dataframe
test['review'] = test['review'].apply(stem_words)
test[['polarity', 'subjectivity']] = test['review'].apply(get_sentiment).apply(pd.Series)
test['labeled_product_id'] = test['fake_asin'] * test ['product_ID']

test.to_csv('test_cleaned.csv',index=False)
